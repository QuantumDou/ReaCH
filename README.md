# TriCL

Code for paper "Diagnostic Reasoning Enhanced Radiology Report Generation via Hierarchical Contrastive Learning"

## ğŸ› ï¸ Pre-trained Weights Setup
- **BiomedCLIP** 
- Purpose: Offline retrieval
  -Source: [Hugging Face](<https://huggingface.co/microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224>)

- **CheXbert**
- Purpose: Validation
  -Source:[GitHub](<https://github.com/stanfordmlgroup/CheXbert>)


-**Directory Structure Example:**
```
checkpoint/
â”œâ”€â”€ biomedclip/       # or biomedclip.pth
â””â”€â”€ chexbert/         # or chexbert.pth
 ```

---

## ğŸ“šDataset Download

### MIMIC-CXR Dataset:
Data Sources:
   â€¢ Images: Download from [Physionet](https://physionet.org/content/mimic-cxr-jpg/2.0.0/).
   â€¢ Annotations: Obtain annotations.json from  [R2Gen](https://github.com/cuhksz-nlp/R2Gen)

   Directory Setup:
    ```
dataset/
â””â”€â”€ mimic_cxr/
    â”œâ”€â”€ images/                   # Place MIMIC-CXR image files here
    â””â”€â”€ annotations.json          # Chen et al. labels
     ```

**Note:** This dataset requires authorization.
   

### IU X-Ray Dataset:
Data Sources:
â€¢ Images & Annotations: Download PNG format images and annotations.json from R2Gen
Directory Setup:
```
dataset/
â””â”€â”€ iu_x-ray/
    â”œâ”€â”€ images/                   # Place IU X-Ray PNG images here
    â””â”€â”€ annotations.json          # Corresponding annotations
```

---

## ğŸ’¡ Execution Steps

Data Preprocessing
Annotation Processing:
Input: annotation.json
Transformation:
Extract radiology reports
Parse disease-organ pairs using the format:
<disease> in <organ>
Generate question-answer pairs by:
Randomly selecting questions from tools/question/
Pairing with corresponding anatomical findings
Implementation:
See tools/generate_graph for detailed processing workflow


3. Model training and validation:
    ```
    python main.py 
    ```
  
---

## Acknowledgements

- See `folder_structure.txt`

